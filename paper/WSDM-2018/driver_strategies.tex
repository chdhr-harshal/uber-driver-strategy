%!TEX root = main.tex

\section{Driver Strategies}
\label{sec:driver_strategies}

In this section, we discuss five kinds of on-demand ride service
driver strategies.
\begin{enumerate}
    \item {\naive} 
    \item {\relocation} 
    \item {\flexible} 
    \item {\relocationflexible}
    \item {\earningsgoal}
\end{enumerate}

In each of the above strategy, a driver has different sets of actions available to choose from at any stage. Our goal is to compare the optimal policies for each of the strategies, allowing us to devise an overall optimal in expectation strategy for a driver.

\subsection{\naive}
In the naive strategy, a driver performs a random walk over the city. At the end of every passenger ride, the driver waits in the current zone for next passenger pickup. Hence, the only allowable action is {\getpassenger}.
\begin{equation}
\actionsset = \{\getpassengeraction\}
\end{equation}
We can calculate cumulative earnings of a driver following a naive strategy at any stage as follows,
\begin{eqnarray}
\label{eq:naive_strategy}
\cumulativeearning{i}{t} &=& \empiricaltransitionmatrix_{i}^{t} (\rewardsmatrix_{i}^{t} +  \inducedearningvector{i}{t}{\getpassengeraction})
\end{eqnarray}
The naive strategy accounts for unsuccessful cases where a driver fails to get a passenger in the current time unit as well. It should be noted that, a driver in this strategy does not have flexibility to choose working times. Hence, $B=N$. This allows us to drop the superscript $b$ from Eq.(\ref{eq:naive_strategy}) without any loss of generality.

\subsection{\relocation}
In the relocation strategy, an idle driver in zone $i$ has two choices viz., {\getpassenger} and {\relocate}. Hence, the set of allowable actions for a driver contains $n$ different actions, one of which is {\getpassenger} and $(n-1)$ {\relocate} actions, one for each different city zone. 
\begin{equation}
\actionsset =  \{\getpassengeraction\} \cup \{\relocateaction | \forall j \in \cityzones, j \neq i \}
\end{equation}
We can further restrict the number of {\relocate} actions by removing from consideration the zones where relocating causes a situation where $t \geq N$.

A driver following relocation strategy chooses the action that maximises {\totalexpectedearnings}. This choice is expressed in recursive manner as follows,
\begin{eqnarray}
\label{eq:relocation_strategy}
\cumulativeearning{i}{t} &=& \max_{a \in \actionsset}
    \begin{cases}
    \empiricaltransitionmatrix_{i}^{t} (\rewardsmatrix_{i}^{t} +  \inducedearningvector{i}{t}{a}), &\textrm{  if } a = \getpassengeraction\\ \\
    \max_{j} \bigg\{r^t(i,j) + \cumulativeearning{j}{t'}\bigg\}, &\textrm{  if } a = \relocateaction
    \end{cases}
\end{eqnarray}
While solving Eq.(\ref{eq:relocation_strategy}), we also get an optimal policy for relocation strategy by keeping track of optimal action choices. Similar to the {\naive}, the driver does not have flexibility to choose work timings, hence we drop the superscript $b$ from Eq.(\ref{eq:relocation_strategy}).

\subsection{\flexible}

In the flexible schedule strategy, a driver has flexibility to decided working times. As a result, we impose an additional constraint of a working time budget $B$ that a driver can split over a finite horizon of $N$ time units. Thus, this strategy aims to figure out an optimal in expectation work schedule for the driver. At any stage, a driver can log out of the on-demand ride service and return to home zone. Hence, the set of allowable actions for a driver contains 2 different actions, {\getpassenger} and {\gohome}. 
\begin{equation}
\actionsset = \{\getpassengeraction, \gohomeaction\}
\end{equation}
A driver following this strategy chooses action that maximises {\totalexpectedearnings}. This choice is expressed in recursive manner as follows,
\begin{eqnarray}
\label{eq:flexible_strategy}
\cumulativeearning{i}{t,b} &=& \max_{a \in \actionsset}
    \begin{cases}
    \empiricaltransitionmatrix_{i}^{t} (\rewardsmatrix_{i}^{t} +  \inducedearningvector{i}{t,b}{a}), &\textrm{  if } a = \getpassengeraction\\ \\
    r^t(i,\homezone) + \cumulativeearning{\homezone}{t',b}, &\textrm{  if } a = \gohomeaction
    \end{cases}
\end{eqnarray}
Similar to the {\relocation}, we can find an optimal in expectation work schedule by keeping track of optimal action choices while solving Eq.(\ref{eq:flexible_strategy}).

\subsection{\relocationflexible}

This is the most general strategy formed by combining the {\relocation} as well as the {\flexible} described earlier. In this strategy, a driver has complete freedom for choices regarding work schedule as well relocation to different zones, in order to maximize {\totalexpectedearnings}. Just like the {\flexible}, a driver has a budget constraint of $B$ time units to be consumed over a finite horizon $N$ time units. An idle driver in zone $i$ following this strategy has folowing set of available choices,
\begin{equation}
\actionsset =  \{\getpassengeraction, \gohomeaction\} \cup \{\relocateaction | \forall j \in \cityzones, j \neq i \}
\end{equation}
Similar to {\relocation}, we restrict the {\relocate} actions to ones which do not result in $t \geq N$ or $b \geq B$.

A driver following the relocation with flexible schedule strategy chooses the action that maximises {\totalexpectedearnings}. This choice can be expressed as follows,
\begin{eqnarray}
\label{eq:relocationflexible_strategy}
\cumulativeearning{i}{t,b} &=& \max_{a \in \actionsset}
    \begin{cases}
    \empiricaltransitionmatrix_{i}^{t} (\rewardsmatrix_{i}^{t} +  \inducedearningvector{i}{t,b}{a}), &\textrm{  if } a = \getpassengeraction\\ \\
    r^t(i,\homezone) + \cumulativeearning{\homezone}{t',b}, &\textrm{  if } a = \gohomeaction \\ \\
    \max_{j} \bigg\{r^t(i,j) + \cumulativeearning{j}{t',b'}\bigg\}, &\textrm{  if } a = \relocateaction
    \end{cases}
\end{eqnarray}
As earlier, we can find an optimal in expectation policy by keeping track of optimal action choices while solving Eq.(\ref{eq:relocationflexible_strategy})

\subsection{\earningsgoal}
In the fixed earnings goal strategy, a driver sets an earnings goal before logging in into the on-demand ride service and drives until fixed goal is reached.
\todo[Harshal]{Write in this section how this strategy solution is a part of the previous strategies already discussed.}
