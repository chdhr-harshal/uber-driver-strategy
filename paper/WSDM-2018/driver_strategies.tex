%!TEX root = main.tex

\section{Driver strategies and optimization algorithms}
\label{sec:driver_strategies}
We now describe the different driver strategies, which are defined based on the
set of actions {\actionsset} that the driver has at his disposal. We also show how to 
optimally solve
the {\originalproblem} problem in polynomial time for different sets {\actionsset}.

For the rest of the section, we will use the following notation: we will denote by 
$\cumulativeearning{i}{b}{t}$ the \emph{total expected future earnings} of a driver who is in zone $i$ at time $t$ with 
budget $b$ time units remaining. Using this notation,
the {\totalexpectedearnings} of a driver can be expressed as $\cumulativeearning{\homezone}{B}{N}$.


If a driver at zone $i$ at time $t$ with $b$ budget units remaining either takes a passenger ride to zone $j$
or relocates to zone $j$, that trip ends at time 
$t' = t + \tau^t(i,j)$ with remaining budget $b' = b - \tau^t(i,j)$. The 
total expected future earnings at that point for the driver 
is: {\cumulativeearning{j}{b'}{t'}. 
For notational convenience we also use
{\inducedearningvector{i}{t,b}} to denote the vector of such
cumulative earnings across different zones $j$ induced when a driver takes an  {\getpassengeraction} 
action.

%%Given the above, 
We now define the driver strategies as well as the
solutions to the instances of the {\originalproblem} problem they induce.


\spara{The {\relocationflexible} strategy:}
This is the most general strategy where a driver has complete freedom for choices regarding work schedule as well relocation to different zones, in order to maximize {\totalexpectedearnings}. More specifically, a driver has a budget constraint of $B$ time units to be consumed over a finite horizon $N$ time units. An idle driver in zone $i$ following this strategy has following set of available choices,
\begin{equation}
\actionsset =  \{\getpassengeraction, \gohomeaction\} \cup \{\relocateaction | \forall j \in \cityzones, j \neq i \}
\end{equation}
Note that we restrict the {\relocate} actions to ones which do not result in $t \geq N$ or $b < 0$.

A driver following the relocation with flexible schedule strategy chooses the action that maximizes {\totalexpectedearnings}. This means that for this strategy, the solution to
the {\originalproblem} problem can be found by solving the following dynamic programming (DP)
recurrence:
\begin{eqnarray}
\label{eq:relocationflexible_strategy}
\cumulativeearning{i}{t}{b} &=& \max_{a \in \actionsset}
    \begin{cases}
    \empiricaltransitionmatrix_{i} (\rewardsmatrix_{i} +  \inducedearningvector{i}{t,b}), &\textrm{  if } a = \getpassengeraction\\ \\
    -cost(i,\homezone) + \cumulativeearning{\homezone}{t'}{b}, &\textrm{  if } a = \gohomeaction \\ \\
    \max_{j} \{-cost(i,j) + \cumulativeearning{j}{t'}{b'}\}, &\textrm{  if } a = \relocateaction
    \end{cases}
\end{eqnarray}
Each of the $O(n N B)$ entries in the output of this dynamic-program involves consideration of at most $O(n)$ actions. 
Hence, the solution to the {\originalproblem} problem can be found in $O(n^2 N B)$ running time.

\spara{Other strategies:} In addition to the general {\relocationflexible} strategy we also
consider the following three special cases to model other plausible strategies of ridesharing drivers: 
the {\naive}, the {\relocation} and the {\flexible} strategies.

In the {\naive} strategy, a driver performs a random walk over the city on weekdays from 9AM-5PM, with locations dictated exclusively 
by the passengers he picks up. At the end of every passenger ride, the driver waits in the current zone for next passenger pickup. 
Hence, the only allowable action is {\getpassenger}.


In the {\flexible} strategy, a driver has flexibility to decide working times, modeling a driver who uses heuristics to decide the most profitable times to work.   As a result, we impose an additional constraint of a working time budget $B$ that a driver can split over a finite horizon of $N$ time units. Thus, this strategy aims to figure out an optimal 
in-expectation work schedule for the driver.  
At any stage, a driver can log out of the on-demand ride service and return to home zone. Hence, the set of allowable actions for a driver contains 2 different actions, {\getpassenger} and {\gohome}.  Thus: $
\actionsset = \{\getpassengeraction, \gohomeaction\}$.

In the {\relocation} strategy, an idle driver in zone $i$ has two choices, viz. {\getpassenger} and {\relocate}. Hence, the set of allowable actions for a driver contains $n$ different actions, one of which is {\getpassenger} and $(n-1)$ {\relocate} actions, one for each different city zone.  Thus: $
\actionsset =  \{\getpassengeraction\} \cup \{\relocateaction | \forall j \in \cityzones, j \neq i \}
$.
We can further restrict the number of {\relocate} actions by removing from consideration the zones where relocating causes a situation where $t \geq N$.

Solving the {\originalproblem} problem for the {\naive}, the {\relocation} and the
{\flexible} strategies can be done by streamlined versions of the DP presented in 
Equation~\eqref{eq:relocationflexible_strategy}; the details
are omitted due to space constraints.


\iffalse
We now consider strategic behavior, modeled through a set of strategies for how 
a rational Uber driver might undertake to maximize profit over the course of a 
40-hour workweek.  

In the {\em naive} baseline strategy, a driver simply drives from 9-5, 
waiting in place for each subsequent ride and returning home at the end of each day.  Next, 
we introduce the ability of a driver to {\em relocate} to any preferred location in the
city before waiting for a subsequent ride.  Then, we allow each driver to maintain a
{\em flexible} schedule, by selectively choosing the specific hours worked (while
keeping the total work week constant).  When considered jointly, a driver who acts
strategically both by relocating and by using a flexible strategy is optimizing both
over time and space (location).  Finally, we consider an entirely different strategy,
modeling drivers who use a naive strategy to work towards an {\em earnings goal}, for
example, driving a five-day workweek, but stopping when they have earned \$100 a day.  Such a
strategy can be compared against the others on a ratable basis, i.e., \$/hour earned.

Formally, we model these driver behaviors as a set of $n+2$ actions $A$ they can potentially
take after completion of each ride: 
\begin{equation}
\mbox{
$A$ = \{{\getpassenger}, {\gohome}, {\relocate} to zone $j$\}
}
\end{equation}

For a driver who may take all the available actions,
acting strategically about relocation and flexible scheduling, she maximizes total expected 
earnings (at time $t$, location $i$, and remaining time budget $b$) over all actions as:

%\begin{eqnarray}
%\label{eq:relocationflexible_strategy}
%\cumulativeearning{i}{t,b} &=& \max %_{a \in \actionsset}
%    \begin{cases}
%    \empiricaltransitionmatrix_{i}^{t} (\rewardsmatrix_{i}^{t} +  \inducedearningvector{i}{t,b}{}), &\textrm{//  {\getpassenger }} \\ \\
%    r^t(i,\homezone) + \cumulativeearning{\homezone}{t',b}, &\textrm{//  {\gohome}} \\ \\ 
%    \max_{j} \bigg\{r^t(i,j) + \cumulativeearning{j}{t',b'}\bigg\} &\textrm{//  {\relocate} to zone $j$}
%    \end{cases}
%\end{eqnarray}

We can find an optimal in expectation policy by reasoning about Eq.(\ref{eq:relocationflexible_strategy}) as a dynamic programming (DP)
recurrence, and thus building up and tracking optimal action choices.  Since each of O$(n T B)$ entries involves consideration of at most
$n+2$ actions, computing the optimal policy takes O($n^2 T B$) time.  Solving for alternative strategic behavior entails similar, but 
simpler, DP approaches.
\fi

\begin{comment}
In the relocation strategy, an idle driver in zone $i$ has two choices viz., {\getpassenger} and 
Hence, the set of allowable actions for a driver contains $n$ different actions, one of which is {\getpassenger} and $(n-1)$ {\relocate} actions, one for each different city zone. 

In the flexible schedule strategy, a driver has flexibility to decided working times. As a result, we impose an additional constraint of a working time budget $B$ that a driver can split over a finite horizon of $N$ time units. Thus, this strategy aims to figure out an optimal in expectation work schedule for the driver. At any stage, a driver can log out of the on-demand ride service and return to home zone. Hence, the set of allowable actions for a driver contains 2 different actions, {\getpassenger} and 


In this section, we discuss five kinds of on-demand ride service
driver strategies.
\begin{enumerate}
    \item {\relocation} 
    \item {\flexible} 
    \item {\relocationflexible}
    \item {\earningsgoal}
\end{enumerate}

In each of the above strategy, a driver has different sets of actions available to choose from at any stage. Our goal is to compare the optimal policies for each of the strategies, allowing us to devise an overall optimal in expectation strategy for a driver.

\subsection{\naive}
In the naive strategy, a driver performs a random walk over the city. At the end of every passenger ride, the driver waits in the current zone for next passenger pickup. Hence, the only allowable action is {\getpassenger}.
\begin{equation}
\end{equation}
We can calculate cumulative earnings of a driver following a naive strategy at any stage as follows,
\begin{eqnarray}
\label{eq:naive_strategy}
\cumulativeearning{i}{t} &=& \empiricaltransitionmatrix_{i}^{t} (\rewardsmatrix_{i}^{t} +  \inducedearningvector{i}{t}{\getpassengeraction})
\end{eqnarray}
The naive strategy accounts for unsuccessful cases where a driver fails to get a passenger in the current time unit as well. It should be noted that, a driver in this strategy does not have flexibility to choose working times. Hence, $B=N$. This allows us to drop the superscript $b$ from Eq.(\ref{eq:naive_strategy}) without any loss of generality.

\subsection{\relocation}
\begin{equation}
In the relocation strategy, an idle driver in zone $i$ has two choices viz., {\getpassenger} and {\relocate}. Hence, the set of allowable actions for a driver contains $n$ different actions, one of which is {\getpassenger} and $(n-1)$ {\relocate} actions, one for each different city zone. 
\actionsset =  \{\getpassengeraction\} \cup \{\relocateaction | \forall j \in \cityzones, j \neq i \}
\end{equation}
We can further restrict the number of {\relocate} actions by removing from consideration the zones where relocating causes a situation where $t \geq N$.

A driver following relocation strategy chooses the action that maximises {\totalexpectedearnings}. This choice is expressed in recursive manner as follows,
\begin{eqnarray}
\label{eq:relocation_strategy}
\cumulativeearning{i}{t} &=& \max_{a \in \actionsset}
    \begin{cases}
    \empiricaltransitionmatrix_{i}^{t} (\rewardsmatrix_{i}^{t} +  \inducedearningvector{i}{t}{a}), &\textrm{  if } a = \getpassengeraction\\ \\
    \max_{j} \bigg\{r^t(i,j) + \cumulativeearning{j}{t'}\bigg\}, &\textrm{  if } a = \relocateaction
    \end{cases}
\end{eqnarray}
While solving Eq.(\ref{eq:relocation_strategy}), we also get an optimal policy for relocation strategy by keeping track of optimal action choices. Similar to the {\naive}, the driver does not have flexibility to choose work timings, hence we drop the superscript $b$ from Eq.(\ref{eq:relocation_strategy}).

\subsection{\flexible}

In the flexible schedule strategy, a driver has flexibility to decided working times. As a result, we impose an additional constraint of a working time budget $B$ that a driver can split over a finite horizon of $N$ time units. Thus, this strategy aims to figure out an optimal in expectation work schedule for the driver. At any stage, a driver can log out of the on-demand ride service and return to home zone. Hence, the set of allowable actions for a driver contains 2 different actions, {\getpassenger} and {\gohome}. 
\begin{equation}
\actionsset = \{\getpassengeraction, \gohomeaction\}
\end{equation}
A driver following this strategy chooses action that maximises {\totalexpectedearnings}. This choice is expressed in recursive manner as follows,
\begin{eqnarray}
\label{eq:flexible_strategy}
\cumulativeearning{i}{t,b} &=& \max_{a \in \actionsset}
    \begin{cases}
    \empiricaltransitionmatrix_{i}^{t} (\rewardsmatrix_{i}^{t} +  \inducedearningvector{i}{t,b}{a}), &\textrm{  if } a = \getpassengeraction\\ \\
    r^t(i,\homezone) + \cumulativeearning{\homezone}{t',b}, &\textrm{  if } a = \gohomeaction
    \end{cases}
\end{eqnarray}
Similar to the {\relocation}, we can find an optimal in expectation work schedule by keeping track of optimal action choices while solving Eq.(\ref{eq:flexible_strategy}).

\subsection{\relocationflexible}

This is the most general strategy formed by combining the {\relocation} as well as the {\flexible} described earlier. In this strategy, a driver has complete freedom for choices regarding work schedule as well relocation to different zones, in order to maximize {\totalexpectedearnings}. Just like the {\flexible}, a driver has a budget constraint of $B$ time units to be consumed over a finite horizon $N$ time units. An idle driver in zone $i$ following this strategy has folowing set of available choices,
\begin{equation}
\actionsset =  \{\getpassengeraction, \gohomeaction\} \cup \{\relocateaction | \forall j \in \cityzones, j \neq i \}
\end{equation}
Similar to {\relocation}, we restrict the {\relocate} actions to ones which do not result in $t \geq N$ or $b \geq B$.

A driver following the relocation with flexible schedule strategy chooses the action that maximises {\totalexpectedearnings}. This choice can be expressed as follows,
\begin{eqnarray}
\label{eq:relocationflexible_strategy}
\cumulativeearning{i}{t,b} &=& \max_{a \in \actionsset}
    \begin{cases}
    \empiricaltransitionmatrix_{i}^{t} (\rewardsmatrix_{i}^{t} +  \inducedearningvector{i}{t,b}{a}), &\textrm{  if } a = \getpassengeraction\\ \\
    r^t(i,\homezone) + \cumulativeearning{\homezone}{t',b}, &\textrm{  if } a = \gohomeaction \\ \\
    \max_{j} \bigg\{r^t(i,j) + \cumulativeearning{j}{t',b'}\bigg\}, &\textrm{  if } a = \relocateaction
    \end{cases}
\end{eqnarray}
As earlier, we can find an optimal in expectation policy by keeping track of optimal action choices while solving Eq.(\ref{eq:relocationflexible_strategy})

\subsection{\earningsgoal}
In the fixed earnings goal strategy, a driver sets an earnings goal before logging in into the on-demand ride service and drives until fixed goal is reached.
\todo[Harshal]{Write in this section how this strategy solution is a part of the previous strategies already discussed.}
\end{comment}
