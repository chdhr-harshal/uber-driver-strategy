%!TEX root = main.tex

\section{Problem Setup}
\label{sec:problem_setup}

In this section, we describe the basics of our problem setup and provide the necessary notations.

\subsection{\textsc{City}}

Throughout the paper, we will assume that a city is divided into non-overlaping set of zones denoted by \cityzones. 
We represent a city in form of a weighted directed graph $G=(\cityzones, E)$ with 
$|\cityzones| = n$ and $|E| = n(n-1) + n$ arcs, an arc between each pair of nodes and self loops on every node. 
Some of the attributes of a city are as follows:

\subsubsection{\sc{EmpiricalTransitionMatrix} (\empiricaltransitionmatrix)}

Every arc $e(i\rightarrow j) \in E$ is associated with a real-valued weight $f(i,j) \in [0,1]$ such that,
\begin{equation}
	\sum_{j \in \cityzones} f(i,j) = 1, \forall i \in \cityzones
\end{equation}
Therefore, the weights give rise to a {\markovchain} with a transition matrix {\empiricaltransitionmatrix} -- 
where $\empiricaltransitionmatrix(i,j) = f(i,j)$ 
denotes the empirically observed probability of a passenger in zone $i$
travelling to zone $j$. We assume that no passenger rides a taxi within the same zone. Hence, as a special case, 
$f(i,i)$ denotes the probability of not finding a passenger in zone $i$. The empirical transition matrix \empiricaltransitionmatrix
changes throughout the day. Hence, we use $\empiricaltransitionmatrix^t$ to denote the matrix at time $t$.

\subsubsection{\sc{TravelTimeMatrix} (\traveltimematrix)}

Every arc $e(i\rightarrow j) \in E$ is also associated with a positive valued weight $\tau(i,j) > 0$ 
denoting the travel time duration for a taxi ride from zone $i$ to zone $j$. 
These weights give us a travel time matrix \traveltimematrix. 
Just like {\empiricaltransitionmatrix},  travel time matrix also evolves throughout the day. 
Hence, we use $\traveltimematrix^t$ to denote the matrix at time $t$.

\subsubsection{\sc{RewardsMatrix} (\rewardsmatrix)}

Every arc $e(i \rightarrow j) \in E$ is also associated with a real valued reward $r(i,j) \geq 0$ denoting
the net reward for a taxi driver in ride from zone $i$ to zone $j$. The net rewards include the driver's
share of earnings from a passenger minus the sundry expenses like gas, vehicle value depreciations, etc.
Following our earlier convention, rewards matrix at time $t$ is denoted by $\rewardsmatrix^t$.

\subsection{\textsc{Driver}}

In our setting, we devise optimal in expectation strategies for an individual taxi driver working for an
on-demand ride service. There is an alloted maximum budget of $B$ time units during which the driver can pick
up passenger rides. However, in some strategies, the driver can flexibly split the working time budget 
over a finite horizon of $N$ time units. Obviously, $N \geq B$. As an example, a driver with a 40 hour work week
shall have $B=40$ hours and $N=168$ hours.

\subsubsection{\sc{HomeZone} (\homezone)}

The driver has a unique home zone denoted by $i_0 \in \cityzones$. It is assumed that a driver begins the finite horizon of $N$
time units from home zone and returns back at the end of driving budget. A driver may also choose to return to 
home zone as a part of some optimal strategy.

\subsubsection{\sc{DriverActions} (\actionsset)}

Whenever faced with making a decision regarding next decision during a driving strategy, a driver can take three kinds
of actions.

\begin{itemize}
	\item {\getpassenger} \getpassengeraction: Wait for a passenger in the current zone. 
	\item {\gohome} \gohomeaction: Log out of the on-demand ride service, go to home zone and wait in it. A driver can
	choose this action only while away from home zone.
	\item {\relocate} \relocateaction: Relocate to city zone $j$.
\end{itemize}

\subsubsection{\sc{DriverPolicy} (\policy)}

A driver policy is an ordered set of time and location dependent actions taken by a driver at different steps of the strategy. As the total number of actions taken by a driver while exhausting the budget $B$ depends on the actual actions, the length
of a driver policy {\policy} varies. 

Each time and location dependent action in the {\policy} can be expressed in form of a 3-tuple -- $(\actualaction, \actuallocation, \actualtime)$
where $\actualaction \in \actionsset$ refers to actual action, $\actuallocation \in \cityzones$ is the zone at which action was taken and $\actualtime \leq N$ is the 
time at which the action was taken.

% A policy of length $L$, $\policy_L = (\mathbf{a}^1, \cdots, \mathbf{a}^L)$, induces
% three ordered sets of length $L$ each.

% \begin{itemize}
% 	\item \textit{Action Trajectory} : An ordered set of actions taken by driver following the policy, denoted by \actiontrajectory{\policy_L}.
% 	\item \textit{Location Trajectory} : An ordered set of nodes at which the driver takes each of the actions from \actiontrajectory{\policy_L}, denoted by \locationtrajectory{\policy_L}.
% 	\item \textit{Time Trajectory} : An ordered set of times at which the driver takes each of the actions from \actiontrajectory{\policy_L}, denoted by \timetrajectory{\policy_L}.
% \end{itemize}

Furthermore, for a policy of length $L$, the corresponding policy space can be denoted by
$\policyspace_L = \actionsset^{nL}$. The set of all possible policies is $\policyspace = \{\policyspace_1, \cdots, \policyspace_N\}$, as the maximum length of a policy can be $N$.


\subsection{\textsc{Calculation of Earnings}}
Now, we describe the attributes of the driver actions described in the previous section.

\subsubsection{\sc{ActionEarning}}

Action earning is a function of action, zone and time. Let \actionearning{i}{t}{a} denote the immediate expected earnings of a driver 
on taking action $a$ while inside zone $i$ at time $t$. Hence, for each of the actions described in the previous section, the
action earning is calculated differently.

\begin{itemize}
	\item {\getpassenger} : For action {\getpassengeraction}, taken inside zone $i$ at time $t$, the action earnings function
	is calculated as,
	\begin{equation}
	\actionearning{i}{t}{a_0} = \empiricaltransitionmatrix_{i}^{t} \bigcdot \rewardsmatrix_{i}^{t}
	\end{equation}
	where $\empiricaltransitionmatrix_{i}^{t}$ and $\rewardsmatrix_{i}^{t}$ denote the $i$-th rows of $\empiricaltransitionmatrix^{t}$ and $\rewardsmatrix^{t}$ respectively. \\

	\item {\gohome} : For action {\gohomeaction}, taken inside zone $i$ at time $t$, the action earnings function is simply
	\begin{equation}
	\actionearning{i}{t}{a_1} = r^t(i,\homezone)
	\end{equation}
	where $r^t(i,\homezone)$ is the corresponding element of $\rewardsmatrix^{t}$. \\

	\item {\relocate} : Action {\relocateaction} taken inside zone $i$ at time $t$ takes the driver to zone $j$. Therefore, the action earnings function is
	\begin{equation}
	\actionearning{i}{t}{a_2(j)} = r^t(i,j)
	\end{equation}
	where $r^t(i,j)$ is the corresponding element of $\rewardsmatrix^{t}$ and $j \neq i$. \\
\end{itemize}

\subsubsection{\sc{CumulativeEarning}}

Cumulative Earning is a function of zone and time. Let \cumulativeearning{i}{t,b} denote the total expected future earnings of a driver
while inside zone $i$ at time $t$ with $b$ budget units consumed. Using this definition, total expected earnings of a driver can be expressed
as \cumulativeearning{\homezone}{N,B}.

\subsubsection{\sc{InducedEarningVector}}

If a driver at zone $i$ at time $t$ with $b$ budget units consumed takes a passenger ride to zone $j$, the rides end at time 
$t' = t + \tau^t(i,j)$ with budget $b' = b + \tau^t(i,j)$ consumed. The \textsc{CumulativeEarning} of the driver in zone $j$ at time $t'$ with $b'$
budget units consumed is \cumulativeearning{j}{t',b'}. Let us denote using \inducedearningvector{i}{t,b}{\getpassengeraction} the vector of such
cumulative earnings across different zones $j$ induced when a driver takes an action {\getpassengeraction} and call it an 
\textsc{InducedEarningVector}. Instead of a vector of cumulative earnings, {\gohome} action induces a single value i.e., \cumulativeearning{\homezone}{t',b}. 
As the driver has logged out of system, the consumed budget units $b$ remained unchanged. {\relocate} action induces a single value of cumulative earnings per destination zone $j$, \cumulativeearning{j}{t',b'}. 
Using this formulation, for each of the driver actions, we can recursively express \textsc{CumulativeEarning} as follows,
\begin{itemize}
	\item {\getpassenger} : For {\getpassengeraction} taken inside zone $i$ at time $t$ with $b$ budget units consumed,
	\begin{eqnarray}
	\cumulativeearning{i}{t,b} &=& \actionearning{i}{t}{\getpassengeraction} + \empiricaltransitionmatrix_{i}^{t} \bigcdot \inducedearningvector{i}{t,b}{\getpassengeraction} \\
	\nonumber\\
	&=& \empiricaltransitionmatrix_{i}^{t} (\rewardsmatrix_{i}^{t} + \empiricaltransitionmatrix_{i}^{t} \bigcdot \inducedearningvector{i}{t,b}{\getpassengeraction})
	\end{eqnarray}
	where $\empiricaltransitionmatrix_{i}^{t}$ and $\rewardsmatrix_{i}^{t}$ denote the $i$-th rows of $\empiricaltransitionmatrix^{t}$ and $\rewardsmatrix^{t}$ respectively. \\

	\item {\gohome} : For {\gohomeaction} taken inside zone $i$ at time $t$ with $b$ budget units consumed,
	\begin{eqnarray}
	\cumulativeearning{i}{t,b} &=& \actionearning{i}{t}{\gohomeaction} + \cumulativeearning{\homezone}{t',b} \\
	\nonumber\\
	&=& r^t(i,\homezone) + \cumulativeearning{\homezone}{t',b} 
	\end{eqnarray}
	where $t' = t + \tau^t(i,\homezone)$. \\

	\item {\relocate} : Action {\relocateaction} taken inside zone $i$ at time $t$ with $b$ budget units consumed takes the driver to zone $j$.
	Hence,
	\begin{eqnarray}
	\cumulativeearning{i}{t,b} &=& \actionearning{i}{t}{\relocateaction} + \cumulativeearning{j}{t',b'} \\
	\nonumber \\
	&=& r^t(i,j) + \cumulativeearning{j}{t',b'}
	\end{eqnarray}
	where $t' = t + \tau^t(i,j)$ and $b' = b + \tau^t(i,j)$. 
\end{itemize}

\subsection{\textsc{Nominal Problem}}
For a given sets of time evolving {\empiricaltransitionmatrix}, {\traveltimematrix} and {\rewardsmatrix}, and driver policy \policy, we can calculate {\totalexpectedearnings} using as \cumulativeearning{\homezone}{N,B}.

\begin{eqnarray}
\mathcal{E}^{N,B}(\policy, \empiricaltransitionmatrix, \traveltimematrix, \rewardsmatrix) &=& \cumulativeearning{\homezone}{N,B} \\
&=& \sum_{\mathbf{a} \in \policy}\actionearning{\actuallocation}{\actualtime}{\actualaction}
\end{eqnarray}
where $\mathcal{E}^{N,B}(\policy, \empiricaltransitionmatrix, \traveltimematrix, \rewardsmatrix)$ denotes {\totalexpectedearnings}. We can define the finite horizon nominal problem to find an optimal policy in a {\mdp},
\begin{equation}
\phi(\policyspace, \empiricaltransitionmatrix, \traveltimematrix, \rewardsmatrix) = \max_{\policy \in \policyspace} \mathcal{E}^{N,B}(\policy, \empiricaltransitionmatrix, \traveltimematrix, \rewardsmatrix)
\end{equation}

When the transition matrices, the travel time matrices and the reward matrices are exactly known, the corresponding nominal problem can be solved using Bellman recursion.