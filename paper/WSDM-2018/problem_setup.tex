%!TEX root = main.tex

\section{Problem setup}
\label{sec:problem_setup}

In this section, we describe the basics of our problem setup and provide the necessary notation.

\iffalse
\subsection{Notation}
\label{sec:notation}

Vectors are denoted with lowercase bold letters (e.g., $\vec{a} = [a(i)]$) and matrices are denoted with uppercase bold letters (e.g., $\matr{A} = [a(i,j)]$). The notation $\vec{1}$ refers to the vector of ones, with size dependent on the context. The short form notation $\vec{A_i}$ refers to the $i$-th row vector of the matrix $\matr{A}$. Let $\Theta_n$ be a set of $n \times n$ right-stochastic transition matrices (non-negative matrices with rows that sum to one). A probability simplex in $\mathbb{R}^n$ is denoted by $\Delta_n = \{\vec{p} \in \mathbb{R}^n_+ : \transpose{p} \vec{1} = 1 \}$.
\fi

\subsection{Modeling the city}

Throughout the paper, we will assume that a city is divided into non-overlapping set of zones denoted 
by \cityzones, and time $t$ runs in discrete time steps.
We represent a city in the form of a complete weighted directed graph $G=(\cityzones, E)$ with 
$|\cityzones| = n$ and $|E| = {n \choose 2}$ edges, where the edge weight on 
edge $e(i\rightarrow j)$ corresponds to the likelihood of a driver currently at 
location $i$ receiving a ride request to location $j$.  Additionally, each edge is
associated with a travel time $\tau(i, j)$, a travel cost, and a reward $r(i,j)$.
In the general formulation of our problem, all of these edge attributes are time-varying,
e.g., the rewards would vary with $t$ as $r^t(i,j)$, but to avoid excess notation, we drop
those superscripts in our following discussion.  These attributes of a city, which we use as 
input to our solver, are specified as follows:



%\subsubsection{\sc{EmpiricalTransitionMatrix} (\empiricaltransitionmatrix)}

\begin{comment} % Relocate to data section
\spara{Count matrix (\countmatrix)}: 
Every edge $e(i\rightarrow j) \in E$ is associated with an
integer-valued weight $c(i,j)$ that denotes the number of requests
at zone $i$ that had node $j$ as their destination.
\end{comment}

\spara{Empirical transition matrix (\empiricaltransitionmatrix)}:
Every edge $e(i\rightarrow j) \in E$ is associated with a transition probability
$f(i, j) \in [0,1]$ such that
$\sum_{j \in \cityzones} f(i,j) = 1$, $\forall i \in \cityzones$.  

%%The count matrix gives rise to the transition matrix {\empiricaltransitionmatrix}.
Since the entries of {\empiricaltransitionmatrix} correspond to probabilites, the weights 
give rise to a {\markovchain} with a transition matrix {\empiricaltransitionmatrix} -- 
where each entry $f(i,j)$ denotes the modeled probability of a passenger in zone $i$
traveling to zone $j$. 
As a special case, 
$f(i,i)$ denotes the probability of a driver not finding a passenger in zone $i$ at a
given time step.

%\subsubsection{\sc{TravelTimeMatrix} (\traveltimematrix)}

\spara{Travel time matrix (\traveltimematrix)}:
Every edge $e(i\rightarrow j) \in E$ is also associated with a value $\tau(i,j) > 0$ 
denoting the travel time duration for a ride from zone $i$ to zone $j$. 
These weights give us a travel time matrix {\traveltimematrix} with entries $\tau(i,j)$. 
%Just like {\empiricaltransitionmatrix},  travel time matrix also evolves throughout the day. 
%Hence, we use $\traveltimematrix^t$ to denote the matrix at time $t$.

%\subsubsection{\sc{RewardsMatrix} (\rewardsmatrix)}

\spara{Rewards matrix (\rewardsmatrix)}:
Every edge $e(i \rightarrow j) \in E$ is also associated with a real valued reward $r(i,j)$ denoting
the net reward for a driver delivering a passenger from zone $i$ to zone $j$. The net rewards include the driver's
share of earnings from a passenger minus the sundry costs like gas, vehicle depreciation, etc.  Since these
earnings and costs vary with mileage and transit time, each entry in the rewards matrix $\rewardsmatrix$ 
is of the form $r(i,j)$ = earnings$(i,j)$ - cost$(i,j)$.
%Following our earlier convention, rewards matrix at time $t$ is denoted by $\rewardsmatrix^t$.

Again, in general, all of the input matrices: %{\countmatrix}, 
{\empiricaltransitionmatrix}, {\traveltimematrix} and {\rewardsmatrix}, 
are time-dependent, i.e., their entries could change throughout the day
%Therefore, at any point in time $t$, the instantiations of these matrices are different. 
%We denote those instantiations by $\countmatrix^t$, $\empiricaltransitionmatrix^t$, $\traveltimematrix^t$ and $\rewardsmatrix^t$ respectively.
%For clarity of exposition we will ignore the time-dependent aspect of the model in what follows.


\subsection{Modeling the driver}
Throughout the paper, we will assume that 
each driver comes with a 
maximum budget of $B$ time units
he is willing to work. During these time units the driver 
 can pick
up passenger rides. Depending on the specific setting, the driver can
work $B$ time units consecutively or split them 
over a finite horizon of $N$ time units. Obviously, $N \geq B$. 
As an example, a driver seeking to optimize a 40 hour work week over
a 7 day week shall have $B=40$ hours and $N=168$ hours.

%\subsubsection{\sc{HomeZone} (\homezone)}

\spara{Home zone (\homezone)}: 
Each driver has a unique home zone denoted by $i_0 \in \cityzones$. 
We always assume that the driver starts from his home zone and returns to it
at the end of each of his shifts.
%\subsubsection{\sc{DriverActions} (\actionsset)}

\spara{Driver actions (\actionsset)}: 
Whenever faced with making a decision regarding next decision during a driving strategy, a driver has $n+2$ possible actions he can take: 
%\begin{itemize}
\squishlist
	\item {\getpassenger} (\getpassengeraction): Wait for a passenger in the current zone. 
	\item {\gohome} (\gohomeaction): Log out of the on-demand ride service, relocate to the home zone (if needed) 
  and stop working.   This action does not consume the driver's budget.
	\item {\relocate} (\relocateaction): Relocate to city zone $j$.  This action consumes the driver's budget.
%\end{itemize}
\squishend

%\subsubsection{\sc{DriverPolicy} (\policy)}

\spara{Driver policy (\policy)}:
A driver policy is an ordered set of time and location dependent actions taken by a driver at different steps of the strategy. As the total number of actions taken by a driver while exhausting the budget $B$ depends on the actual actions, the length
of a driver policy {\policy} varies. 

Each time and location dependent action $\mathbf{a}$ in the {\policy} can be expressed in form of a 3-tuple -- $(\actuallocation, \actualtime, \actualaction)$
where $\actualaction \in \actionsset$ refers to actual action, $\actuallocation \in \cityzones$ is the zone at which action was taken and $\actualtime \leq N$ is the 
time at which the action was taken.

% A policy of length $L$, $\policy_L = (\mathbf{a}^1, \cdots, \mathbf{a}^L)$, induces
% three ordered sets of length $L$ each.

% \begin{itemize}
% 	\item \textit{Action Trajectory} : An ordered set of actions taken by driver following the policy, denoted by \actiontrajectory{\policy_L}.
% 	\item \textit{Location Trajectory} : An ordered set of nodes at which the driver takes each of the actions from \actiontrajectory{\policy_L}, denoted by \locationtrajectory{\policy_L}.
% 	\item \textit{Time Trajectory} : An ordered set of times at which the driver takes each of the actions from \actiontrajectory{\policy_L}, denoted by \timetrajectory{\policy_L}.
% \end{itemize}

Furthermore, for a policy of length $L$, the corresponding policy space can be denoted by
$\policyspace_L = \actionsset^{nL}$. The set of all possible policies is $\policyspace = \{\policyspace_1, \cdots, \policyspace_N\}$, as the maximum length of a policy can be $N$.


\subsection{Computing driver earnings}
In this section, we describe the computation of the expected earnings of a driver
who at a specific time $t$ is in zone $i$ and takes action $a$. We denote this by 
$\actionearning{i}{t}{a}$ and depending on the action $a$ it is computed as follows.

\squishlist
	\item For action {\getpassengeraction} ({\getpassenger}), taken inside zone $i$ at time $t$, the action earnings function
	is calculated as an expectation over possible rides,
	\begin{equation}\label{eq:a0}
	\actionearning{i}{t}{a_0} = \empiricaltransitionmatrix_{i}\bigcdot \rewardsmatrix_{i}
	\end{equation}
	where $\empiricaltransitionmatrix_{i}$ and $\rewardsmatrix_{i}$ denote the $i$-th rows of $\empiricaltransitionmatrix$ and $\rewardsmatrix$ respectively. 
	
	\item For action {\gohomeaction} ({\gohome}), taken inside zone $i$ at time $t$, the action earnings function is simply
	\begin{equation}\label{eq:a1}
	\actionearning{i}{t}{a_1} = - cost(i,\homezone)
	\end{equation}
	where we incur a negative reward due to the absence of a paying customer. 

	\item Action {\relocateaction} ({\relocate}), taken inside zone $i$ at time $t$, 
	takes the driver to zone $j \neq i$. Therefore, the action earnings function is 
	\begin{equation}\label{eq:a2}
	\actionearning{i}{t}{a_2(j)} = - cost(i,j)
	\end{equation}
	where the driver again incurs a negative reward due to the absence of a paying customer. 
%\end{itemize}
\squishend

%\subsubsection{\sc{CumulativeEarning}}
\iffalse
\spara{\sc{CumulativeEarning}}:
Cumulative Earning is a function of zone and time. Let {\cumulativeearning{i}{t,b}} denote the total expected future earnings of a driver
while inside zone $i$ at time $t$ with $b$ budget units consumed. Using this definition, total expected earnings of a driver can be expressed
as {\cumulativeearning{\homezone}{N,B}}.

%\subsubsection{\sc{InducedEarningVector}}
\spara{\sc{InducedEarningVector}}:
If a driver at zone $i$ at time $t$ with $b$ budget units consumed takes a passenger ride to zone $j$, the rides end at time 
$t' = t + \tau(i,j)$ with budget $b' = b + \tau(i,j)$ consumed. The \textsc{CumulativeEarning} of the driver in zone $j$ at time $t'$ with $b'$
budget units consumed is {\cumulativeearning{j}{t',b'}}. Let us denote using {\inducedearningvector{i}{t,b}{\getpassengeraction}} the vector of such
cumulative earnings across different zones $j$ induced when a driver takes an action {\getpassengeraction} and call it an 
\textsc{InducedEarningVector}. Instead of a vector of cumulative earnings, {\gohome} action induces a single value i.e., {\cumulativeearning{\homezone}{t',b}}. 
As the driver has logged out of system, the consumed budget units $b$ remained unchanged. {\relocate} action induces a single value of cumulative earnings per destination zone $j$, {\cumulativeearning{j}{t',b'}}. 
Using this formulation, for each of the driver actions, we can recursively express \textsc{CumulativeEarning} as follows,
\begin{itemize}
	\item {\getpassenger} : For {\getpassengeraction} taken inside zone $i$ at time $t$ with $b$ budget units consumed,
	\begin{eqnarray}
	\cumulativeearning{i}{t,b} &=& \actionearning{i}{t}{\getpassengeraction} + \empiricaltransitionmatrix_{i}^{t} \bigcdot \inducedearningvector{i}{t,b}{\getpassengeraction} \\
	\nonumber\\
	&=& \empiricaltransitionmatrix_{i}^{t} (\rewardsmatrix_{i}^{t} + \inducedearningvector{i}{t,b}{\getpassengeraction}) \label{eq:cumulative_earning_get_passenger}
	\end{eqnarray}
	where $\empiricaltransitionmatrix_{i}^{t}$ and $\rewardsmatrix_{i}^{t}$ denote the $i$-th rows of $\empiricaltransitionmatrix^{t}$ and $\rewardsmatrix^{t}$ respectively. \\

	\item {\gohome} : For {\gohomeaction} taken inside zone $i$ at time $t$ with $b$ budget units consumed,
	\begin{eqnarray}
	\cumulativeearning{i}{t,b} &=& \actionearning{i}{t}{\gohomeaction} + \cumulativeearning{\homezone}{t',b} \\
	\nonumber\\
	&=& r^t(i,\homezone) + \cumulativeearning{\homezone}{t',b} 
	\end{eqnarray}
	where $t' = t + \tau(i,\homezone)$. \\

	\item {\relocate} : Action {\relocateaction} taken inside zone $i$ at time $t$ with $b$ budget units consumed takes the driver to zone $j$.
	Hence,
	\begin{eqnarray}
	\cumulativeearning{i}{t,b} &=& \actionearning{i}{t}{\relocateaction} + \cumulativeearning{j}{t',b'} \\
	\nonumber \\
	&=& r^t(i,j) + \cumulativeearning{j}{t',b'}
	\end{eqnarray}
	where $t' = t + \tau(i,j)$ and $b' = b + \tau(i,j)$. 
\end{itemize}
\fi

\subsection{Problem definition}
For given input specification matrices {\empiricaltransitionmatrix}, {\traveltimematrix} and {\rewardsmatrix}, 
as well as the driver's budget $B$,
the \emph{total expected earnings} of a driver  with policy $\policy$ is simply:

\begin{equation}\label{eq:totalexpectedearnings}
\mathcal{E} (\policy,\empiricaltransitionmatrix,\traveltimematrix,\rewardsmatrix,B) = \sum_{(\actuallocation,\actualtime,\actualaction)\in\policy}\actionearning{\actuallocation}{\actualtime}{\actualaction},
\end{equation}
where $\actionearning{\actuallocation}{\actualtime}{\actualaction}$ is computed
using the Equations~\eqref{eq:a0},~\eqref{eq:a1} and~\eqref{eq:a2} we defined above.

As we seek to maximize the \emph{total expected earnings} of the driver,
we aim to solve the following optimization problem.

\begin{problem}[{\originalproblem}]\label{problem:theproblem}
Given sets of time-evolving {\empiricaltransitionmatrix}, {\traveltimematrix} and {\rewardsmatrix}, as well as the driver's budget $B$,
find a $\policy^\ast$
such that:
\[
\policy^\ast = \argmax_{\policy\in \policyspace}\mathcal{E}(\policy,\empiricaltransitionmatrix,\traveltimematrix,\rewardsmatrix,B).
\]
%% REDUNDANT -- is maximized.
\end{problem}

%When the transition matrices, the travel time matrices and the reward matrices are exactly known, the above problem can be solved optimally using dynamic programming. We discuss this in the next section.
